{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RaduSima/SSL_Project2024/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTDO-OnwpYbJ"
   },
   "outputs": [],
   "source": [
    "# ! pip install datasets\n",
    "# ! pip install -U accelerate\n",
    "# ! pip install -U transformers\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import accelerate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from time import time as tm\n",
    "\n",
    "\n",
    "import gc\n",
    "import numpy\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVfumPq-pu6Q"
   },
   "outputs": [],
   "source": [
    "class OrdinalRegressionHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Ordinal regression head for classification problems. The way it works is by having a single layer that outputs a single value, which is then added to a learnable bias. The output is then passed through a sigmoid function.\n",
    "    The bias is a learnable parameter that is used to shift the output of the single layer to the desired range. It will be learned to have descending order.\n",
    "\n",
    "    class = sum(b_i > 0.5) + 1\n",
    "\n",
    "    class 1: 0, 0, 0, 0\n",
    "    class 2: 1, 0, 0, 0\n",
    "    class 3: 1, 1, 0, 0\n",
    "    class 4: 1, 1, 1, 0\n",
    "    class 5: 1, 1, 1, 1\n",
    "    and so on ...\n",
    "\n",
    "    TODO: should have more than one fc layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, num_classes, intermediate_layers=None):\n",
    "        \"\"\"\n",
    "        The constructor for OrdinalRegressionHead class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            The number of input features. This is the number of features of the output layer of the big model.\n",
    "        num_classes : int\n",
    "            The number of classes in the classification problem.\n",
    "        \"\"\"\n",
    "        super(OrdinalRegressionHead, self).__init__()\n",
    "\n",
    "        if intermediate_layers is None:\n",
    "            intermediate_layers = []\n",
    "\n",
    "        input_size = in_features\n",
    "        layers = []\n",
    "        for layer_size in intermediate_layers:\n",
    "            layers.append(torch.nn.Linear(input_size, layer_size))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            input_size = layer_size\n",
    "        layers.append(torch.nn.Linear(input_size, 1, bias=False))\n",
    "\n",
    "        self.fc = torch.nn.Sequential(*layers)\n",
    "\n",
    "        self.b = torch.nn.Parameter(torch.zeros(num_classes - 1))\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward method for OrdinalRegressionHead class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tensor\n",
    "            the input tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple(tensor, tensor)\n",
    "            The logits and the output of the model.\n",
    "            The logits are useful in the training phase, as BCEWithLogitsLoss is used.\n",
    "        \"\"\"\n",
    "        x = self.fc(x)\n",
    "        y = x + self.b\n",
    "        return y, self.activation(y)\n",
    "\n",
    "class OrdinalRegressionClassifier(torch.nn.Module):\n",
    "    def __init__(self, embeddings_size, num_classes, intermediate_layers=None) -> None:\n",
    "        super(OrdinalRegressionClassifier, self).__init__()\n",
    "        self.head = OrdinalRegressionHead(embeddings_size, num_classes, intermediate_layers)\n",
    "        self.loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        logits, output = self.head(embeddings)\n",
    "        return self.loss(logits, labels), output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKBpibKWpl7b"
   },
   "outputs": [],
   "source": [
    "class OurClassifierModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    OurClassifierModel class is a class for the model that uses the BigBird model and the OrdinalRegressionHead.\n",
    "    It uses the pretrained classifier of a BigBird model and adds an ordinal regression head on top of it.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer, embedding_size=768, num_classes=5, intermediate_layers=None):\n",
    "        super(OurClassifierModel, self).__init__()\n",
    "\n",
    "        self.transformer = transformer\n",
    "        # The output of the transformer model is 768, as it is the output of the last hidden state.\n",
    "        self.classifier = OrdinalRegressionHead(embedding_size, num_classes, intermediate_layers=intermediate_layers)\n",
    "\n",
    "        self.loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        \"\"\"\n",
    "        The forward method for OurClassifierModel class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : tensor\n",
    "            The input tensor, used for transformer.\n",
    "        attention_mask : tensor\n",
    "            The attention mask tensor, used for transformer.\n",
    "        labels : tensor\n",
    "            The target labels for the classification problem.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple(tensor, tensor)\n",
    "            The loss and the output of the model.\n",
    "            The loss is useful in the training phase, as we are using Trainer from HuggingFace.\n",
    "        \"\"\"\n",
    "        x = self.transformer(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        logits, output = self.classifier(x)\n",
    "        return self.loss(logits, labels), output\n",
    "\n",
    "class EmbeddingBigBirdModel(torch.nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(EmbeddingBigBirdModel, self).__init__()\n",
    "        self.bert = bert\n",
    "        # The output of the bert model is 768, as it is the output of the last hidden state.\n",
    "        return\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        The forward method for OurBigBirdModel class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : tensor\n",
    "            The input tensor, used for bert.\n",
    "        attention_mask : tensor\n",
    "            The attention mask tensor, used for bert.\n",
    "        labels : tensor\n",
    "            The target labels for the classification problem.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple(tensor, tensor)\n",
    "            The loss and the output of the model.\n",
    "            The loss is useful in the training phase, as we are using Trainer from HuggingFace.\n",
    "        \"\"\"\n",
    "        return self.bert(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BigBirdForSequenceClassification, BigBirdTokenizer, T5ForSequenceClassification, T5Tokenizer\n",
    "transformers_repo = [\n",
    "  \"google/bigbird-roberta-base\",\n",
    "  \"google/bigbird-roberta-large\",\n",
    "  \"albert-base-v2\",\n",
    "  \"albert-large-v2\",\n",
    "  \"allenai/longformer-base-4096\",\n",
    "  \"t5-small\",\n",
    "  \"t5-base\",\n",
    "  \"t5-large\",\n",
    "  \"facebook/bart-base\",\n",
    "  \"facebook/bart-large\",\n",
    "  \"microsoft/deberta-base\",\n",
    "  \"microsoft/deberta-large\",\n",
    "]\n",
    "\n",
    "transformers_classes = {\n",
    "  \"google/bigbird-roberta-base\": {\n",
    "    \"model_class\": BigBirdForSequenceClassification,\n",
    "    \"tokenizer_class\": BigBirdTokenizer,\n",
    "    \"embedding_size\": 768,\n",
    "    },\n",
    "  \"t5-small\": {\n",
    "    \"model_class\": T5ForSequenceClassification,\n",
    "    \"tokenizer_class\": T5Tokenizer,\n",
    "    \"embedding_size\": 512,\n",
    "    },\n",
    "  \"t5-base\": {\n",
    "    \"model_class\": T5ForSequenceClassification,\n",
    "    \"tokenizer_class\": T5Tokenizer,\n",
    "    \"embedding_size\": 512,\n",
    "    },\n",
    "  \"t5-large\": {\n",
    "    \"model_class\": T5ForSequenceClassification,\n",
    "    \"tokenizer_class\": T5Tokenizer,\n",
    "    \"embedding_size\": 512,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3T_Y7UFp3Gl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def remove_percentage(df, percent):\n",
    "    \"\"\"\n",
    "    Useful method for development purposes. It removes a percentage of the rows from the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to remove rows from.\n",
    "    percent : float\n",
    "        The percentage of rows to remove.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        The dataframe with the rows removed.\n",
    "    \"\"\"\n",
    "    if percent == 0:\n",
    "        return df\n",
    "    numpy.random.seed(42)\n",
    "    num_rows_to_remove = int(len(df) * percent)\n",
    "    df_removed = df.sample(frac=1).iloc[num_rows_to_remove:]\n",
    "    return df_removed\n",
    "\n",
    "\n",
    "def convert_label_to_one_hot_encodings(labels: list[float], num_classes, max_label=3500):\n",
    "    \"\"\"\n",
    "    Convert the labels to a class representation. The class representation is a one-hot encoding of the labels.\n",
    "    This preparation is for the ordinal regression problem.\n",
    "\n",
    "    If the one-hot encoding of the label 3 for multi-class classification is [0, 0, 1, 0, 0],\n",
    "      then the one-hot encoding of label 3 for ordinal regression is [1, 1, 0, 0].\n",
    "\n",
    "    To convert from multi-class to ordinal regression, we can do the following trick:\n",
    "        - compute the one-hot encoding of the labels for multi-class classification\n",
    "        - replace all the 0s with 1s until the first 1 is found (for multi-class classification a 1 is found at the index of the class)\n",
    "        - remove the first column of the one-hot encoding (the first 1)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list[float]\n",
    "        The list of labels to convert.\n",
    "    num_classes : int\n",
    "        The number of classes in the classification problem.\n",
    "    max_label : int, optional\n",
    "        The maximum label in the dataset. This is used to normalize the labels. The default is 2800.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The tensor of the one-hot encoding of the labels.\n",
    "    \"\"\"\n",
    "    # reminder: in ordinal regression, class = sum(output > 0.5) + 1\n",
    "\n",
    "    # normalize the labels\n",
    "    labels = numpy.array(labels) / max_label\n",
    "    class_labels = numpy.zeros((len(labels), num_classes - 1))\n",
    "    for i, label in enumerate(labels):\n",
    "        class_labels[i] = numpy.array(\n",
    "            [1 if j / num_classes <= label else 0 for j in range(1, num_classes)])\n",
    "    return torch.tensor(class_labels)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute the metrics for the model. The metrics are accuracy, recall, precision, f1, and neighborhood accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : object\n",
    "        The predictions of the model, wrapped in an object by Hugging Face trainer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The dictionary of the metrics.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "\n",
    "    return _compute_metrics(preds, labels)\n",
    "\n",
    "def _compute_metrics(preds, labels):\n",
    "    threshold = 0.5\n",
    "    target_class = numpy.sum(labels > threshold, axis=-1) + 1\n",
    "    output_class = numpy.sum(preds > threshold, axis=-1) + 1\n",
    "\n",
    "    # compute accuracy, recall, precision, f1 for threshold 0.5\n",
    "    accuracy = accuracy_score(target_class, output_class)\n",
    "    recall = recall_score(target_class, output_class, average='macro', zero_division=0)\n",
    "    precision = precision_score(target_class, output_class, average='macro', zero_division=0)\n",
    "    f1 = f1_score(target_class, output_class, average='macro', zero_division=0)\n",
    "\n",
    "    # compute neighborhood accuracy -- consider accurate all predictions that are off by 1\n",
    "    neighborhood_accuracy = numpy.sum(\n",
    "        numpy.abs(target_class - output_class) <= 1).item() / (len(labels) * 1.0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"neighborhood_accuracy\": neighborhood_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def get_embedding(model, encoding):\n",
    "    \"\"\"\n",
    "    Get the embedding from the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model to get the embeddings from.\n",
    "    encoding : dict\n",
    "        The encoding of the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The tensor of the embeddings.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = torch.tensor(encoding[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(encoding[\"attention_mask\"]).to(device)\n",
    "\n",
    "    batch_size = 10\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    start_time = tm()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(input_ids), batch_size):\n",
    "            batch_input_ids = input_ids[i:i+batch_size]\n",
    "            batch_attention_mask = attention_mask[i:i+batch_size]\n",
    "            embedding = model(batch_input_ids, attention_mask=batch_attention_mask).to(\"cpu\")\n",
    "            embeddings.append(embedding)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            elapsed_time = int(tm() - start_time)\n",
    "            time_per_iter = round(elapsed_time / (i+batch_size), 2)\n",
    "            remaining_time = int((len(input_ids)-i)*time_per_iter)\n",
    "            print(f\"Status {i+batch_size}/{len(input_ids)}  Elapsed: {elapsed_time}s ({time_per_iter}s/it)  Remaining: {remaining_time}s\", end=\"\\r\")\n",
    "    print(\"Embeddings done.\")\n",
    "    del input_ids\n",
    "    del attention_mask\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def save_embedding(embeddings, filename):\n",
    "    \"\"\"\n",
    "    Save the embeddings to a file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : tensor\n",
    "        The tensor of the embeddings.\n",
    "    filename : str\n",
    "        The filename to save the embeddings to.\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pkl.dump(embeddings, f)\n",
    "\n",
    "def load_embedding(filename):\n",
    "    \"\"\"\n",
    "    Load the embeddings from a file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The filename to load the embeddings from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The tensor of the embeddings.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        embeddings = pkl.load(f)\n",
    "    return embeddings\n",
    "\n",
    "def prepare_dataset(embeddings, labels, num_classes=5, max_label=3500):\n",
    "    labels_tensor = convert_label_to_one_hot_encodings(labels, num_classes, max_label=max_label)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    labels_tensor = labels_tensor.to(device)\n",
    "    embeddings = embeddings.to(device)\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "        \"embeddings\": embeddings,\n",
    "        \"labels\": labels_tensor\n",
    "        })\n",
    "    return dataset\n",
    "\n",
    "def prepare_finetune_dataset(transformer_name, texts, labels, num_classes=5, max_label=3500):\n",
    "    labels_tensor = convert_label_to_one_hot_encodings(labels, num_classes, max_label=max_label)\n",
    "    \n",
    "    tokenizer = transformers_classes[transformer_name][\"tokenizer_class\"].from_pretrained(transformer_name)\n",
    "    embeddings = tokenizer(texts, truncation=True, padding=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "        \"input_ids\": embeddings[\"input_ids\"],\n",
    "        \"attention_mask\": embeddings[\"attention_mask\"],\n",
    "        \"labels\": labels_tensor\n",
    "        })\n",
    "    return dataset\n",
    "\n",
    "def _compute_full_embeddings(transformer_name, texts, set_name, base_path):\n",
    "    \n",
    "    tokenizer = transformers_classes[transformer_name][\"tokenizer_class\"].from_pretrained(transformer_name)\n",
    "    model = transformers_classes[transformer_name][\"model_class\"].from_pretrained(transformer_name)\n",
    "\n",
    "    train_encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "\n",
    "    embedding_model = EmbeddingBigBirdModel(model.bert)\n",
    "    embedding_model.eval()\n",
    "\n",
    "    train_embeddings = get_embedding(embedding_model, train_encodings)\n",
    "\n",
    "    transformer_name_path = transformer_name.replace(\"/\", \"_\")\n",
    "    save_embedding(train_embeddings.cpu().numpy(), f\"{base_path}/{transformer_name_path}_{set_name}_embeddings.pkl\")\n",
    "    return train_embeddings\n",
    "\n",
    "def maybe_load_embeddings(transformer_name, texts, set_name, base_path):\n",
    "    transformer_name_path = transformer_name.replace(\"/\", \"_\")\n",
    "    embeddings_filename = f\"{base_path}/{transformer_name_path}_{set_name}_embeddings.pkl\"\n",
    "    \n",
    "    if os.path.exists(embeddings_filename):\n",
    "        embeddings = load_embedding(embeddings_filename)\n",
    "        embeddings = torch.tensor(embeddings)\n",
    "    else:\n",
    "        embeddings = _compute_full_embeddings(transformer_name, texts, set_name, base_path)\n",
    "    return embeddings\n",
    "\n",
    "def get_model_class_from_name(name):\n",
    "    if name == \"OurClassifierModel\":\n",
    "        return OurClassifierModel\n",
    "    elif name == \"OrdinalRegressionClassifier\":\n",
    "        return OrdinalRegressionClassifier\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SOrHoB1JpOwX",
    "outputId": "3b9b17fe-1da5-4d07-fb4d-98d5750abd9e"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/AMT10/AMT10_train.csv')\n",
    "val_data = pd.read_csv('./data/AMT10/AMT10_validation.csv')\n",
    "test_data = pd.read_csv('./data/AMT10/AMT10_test.csv')\n",
    "\n",
    "train_texts, train_labels = train_data['description'].tolist(), train_data['rating'].tolist()\n",
    "val_texts, val_labels = val_data['description'].tolist(), val_data['rating'].tolist()\n",
    "test_texts, test_labels = test_data['description'].tolist(), test_data['rating'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_finetune(transformer_name, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, intermediate_layers, num_classes):\n",
    "    base_model = transformers_classes[transformer_name][\"model_class\"].from_pretrained(transformer_name)\n",
    "    embedding_size = transformers_classes[transformer_name][\"embedding_size\"]\n",
    "    \n",
    "    train_dataset=prepare_finetune_dataset(transformer_name, train_texts, train_labels, num_classes=num_classes)\n",
    "    val_dataset=prepare_finetune_dataset(transformer_name, val_texts, val_labels, num_classes=num_classes)\n",
    "    test_dataset=prepare_finetune_dataset(transformer_name, test_texts, test_labels, num_classes=num_classes)\n",
    "\n",
    "    model = OurClassifierModel(base_model.bert, embedding_size, num_classes, intermediate_layers)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        learning_rate=1e-4,\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        metric_for_best_model=\"neighborhood_accuracy\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    eval_results = trainer.evaluate(test_dataset)\n",
    "    print(eval_results)\n",
    "\n",
    "    transformer_name_path = transformer_name.replace(\"/\", \"_\")\n",
    "    torch.save(trainer.model.state_dict(), f\"./models/{transformer_name_path}_classifier.pth\")\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grid_search(transformer_name, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, metric_compare_grid_results, param_grid):\n",
    "  train_embeddings = maybe_load_embeddings(transformer_name, train_texts, \"train\", \"./data/AMT10\")\n",
    "  val_embeddings = maybe_load_embeddings(transformer_name, val_texts, \"val\", \"./data/AMT10\")\n",
    "  test_embeddings = maybe_load_embeddings(transformer_name, test_texts, \"test\", \"./data/AMT10\")\n",
    "\n",
    "  param_combinations = list(itertools.product(*param_grid.values()))\n",
    "  embedding_size = transformers_classes[transformer_name][\"embedding_size\"]\n",
    "\n",
    "  best_eval_metric = float('-inf')\n",
    "  best_params = None\n",
    "  best_trainer = None\n",
    "\n",
    "  for params in param_combinations:\n",
    "      params = dict(zip(param_grid.keys(), params))\n",
    "      learning_rate = params['learning_rate']\n",
    "      train_batch_size = params['per_device_train_batch_size']\n",
    "      num_epochs = params['num_train_epochs']\n",
    "      model_class = get_model_class_from_name(params['model_class'])\n",
    "      num_classes = params['num_classes']\n",
    "      intermediate_layers = params['intermediate_layers']\n",
    "      train_metric = params['train_metric']\n",
    "      \n",
    "      to_train_model = model_class(embeddings_size=embedding_size, num_classes=num_classes, intermediate_layers=intermediate_layers)\n",
    "\n",
    "      train_dataset=prepare_dataset(train_embeddings, train_labels, num_classes=num_classes)\n",
    "      val_dataset=prepare_dataset(val_embeddings, val_labels, num_classes=num_classes)\n",
    "      test_dataset=prepare_dataset(test_embeddings, test_labels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "      training_args = TrainingArguments(\n",
    "      learning_rate=learning_rate,\n",
    "      output_dir=\"./results\",\n",
    "      num_train_epochs=num_epochs,\n",
    "      per_device_train_batch_size=train_batch_size,\n",
    "      per_device_eval_batch_size=train_batch_size,\n",
    "      warmup_steps=500,\n",
    "      weight_decay=0.01,\n",
    "      logging_dir=\"./logs\",\n",
    "      metric_for_best_model=train_metric\n",
    "      )\n",
    "\n",
    "      trainer = Trainer(\n",
    "          model=to_train_model,\n",
    "          args=training_args,\n",
    "          train_dataset=train_dataset,\n",
    "          eval_dataset=val_dataset,\n",
    "          compute_metrics=compute_metrics\n",
    "      )\n",
    "\n",
    "      trainer.train()\n",
    "\n",
    "      eval_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "      eval_metric = eval_results[f'eval_{metric_compare_grid_results}']\n",
    "\n",
    "      if eval_metric > best_eval_metric:\n",
    "          best_eval_metric = eval_metric\n",
    "          best_params = params\n",
    "          best_trainer = trainer\n",
    "\n",
    "  print(f\"Best Evaluation Metric: {metric_compare_grid_results}: {best_eval_metric}\")\n",
    "  print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "  # Evaluate the model on the testing set\n",
    "  test_dataset=prepare_dataset(test_embeddings, test_labels, num_classes=best_params['num_classes'])\n",
    "  best_eval_results = best_trainer.evaluate(test_dataset)\n",
    "  print(best_eval_results)\n",
    "\n",
    "\n",
    "  transformer_name_path = transformer_name.replace(\"/\", \"_\")\n",
    "  torch.save(best_trainer.model.state_dict(), f\"./models/{transformer_name_path}_ordinal_regression_model.pth\")\n",
    "  return best_params, best_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [\n",
    "        1e-4,\n",
    "    ],\n",
    "    'per_device_train_batch_size': [\n",
    "        256 # could be bigger (1024, 2048)\n",
    "    ],\n",
    "    'num_train_epochs': [\n",
    "        200, # could be bigger (500, 1000)\n",
    "    ],\n",
    "    \"train_metric\": [\n",
    "        \"neighborhood_accuracy\",\n",
    "        \"accuracy\",\n",
    "    ],\n",
    "\n",
    "    # Model\n",
    "    'model_class': [\n",
    "        'OrdinalRegressionClassifier',\n",
    "    ],\n",
    "    'num_classes': [\n",
    "        5,\n",
    "        10,\n",
    "        35\n",
    "    ],\n",
    "    'intermediate_layers': [\n",
    "        [256, 128, 64, 32],\n",
    "        [512, 128, 32],\n",
    "        [256, 64, 16],\n",
    "    ],\n",
    "}\n",
    "\n",
    "transformer_name = transformers_repo[0] # google/bigbird-roberta-base\n",
    "\n",
    "best_params, best_eval_results = train_grid_search(transformer_name, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, \"neighborhood_accuracy\", param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_eval_results = train_and_finetune(transformer_name, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, best_params['intermediate_layers'], best_params['num_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load models saved on disk and evaluate them on the test set, post training"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPOki/b/VQ8ups8qMwp7W9q",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
