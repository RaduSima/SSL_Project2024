{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RaduSima/SSL_Project2024/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zTDO-OnwpYbJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ! pip install datasets\n",
    "# ! pip install -U accelerate\n",
    "# ! pip install -U transformers\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import accelerate\n",
    "from transformers import BigBirdForSequenceClassification, BigBirdTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from time import time as tm\n",
    "\n",
    "\n",
    "import gc\n",
    "import numpy\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZVfumPq-pu6Q"
   },
   "outputs": [],
   "source": [
    "class OrdinalRegressionHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Ordinal regression head for classification problems. The way it works is by having a single layer that outputs a single value, which is then added to a learnable bias. The output is then passed through a sigmoid function.\n",
    "    The bias is a learnable parameter that is used to shift the output of the single layer to the desired range. It will be learned to have descending order.\n",
    "\n",
    "    class = sum(b_i > 0.5) + 1\n",
    "\n",
    "    class 1: 0, 0, 0, 0\n",
    "    class 2: 1, 0, 0, 0\n",
    "    class 3: 1, 1, 0, 0\n",
    "    class 4: 1, 1, 1, 0\n",
    "    class 5: 1, 1, 1, 1\n",
    "    and so on ...\n",
    "\n",
    "    TODO: should have more than one fc layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, num_classes, intermediate_layers=None):\n",
    "        \"\"\"\n",
    "        The constructor for OrdinalRegressionHead class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            The number of input features. This is the number of features of the output layer of the big model.\n",
    "        num_classes : int\n",
    "            The number of classes in the classification problem.\n",
    "        \"\"\"\n",
    "        super(OrdinalRegressionHead, self).__init__()\n",
    "\n",
    "        if intermediate_layers is None:\n",
    "            intermediate_layers = []\n",
    "\n",
    "        input_size = in_features\n",
    "        layers = []\n",
    "        for layer_size in intermediate_layers:\n",
    "            layers.append(torch.nn.Linear(input_size, layer_size))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            input_size = layer_size\n",
    "        layers.append(torch.nn.Linear(input_size, 1, bias=False))\n",
    "\n",
    "        self.fc = torch.nn.Sequential(*layers)\n",
    "\n",
    "        self.b = torch.nn.Parameter(torch.zeros(num_classes - 1))\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward method for OrdinalRegressionHead class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tensor\n",
    "            the input tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple(tensor, tensor)\n",
    "            The logits and the output of the model.\n",
    "            The logits are useful in the training phase, as BCEWithLogitsLoss is used.\n",
    "        \"\"\"\n",
    "        x = self.fc(x)\n",
    "        y = x + self.b\n",
    "        return y, self.activation(y)\n",
    "\n",
    "class OrdinalRegressionClassifier(torch.nn.Module):\n",
    "    def __init__(self, embeddings_size, num_classes, intermediate_layers=None) -> None:\n",
    "        super(OrdinalRegressionClassifier, self).__init__()\n",
    "        self.head = OrdinalRegressionHead(embeddings_size, num_classes, intermediate_layers)\n",
    "        self.loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        logits, output = self.head(embeddings)\n",
    "        return self.loss(logits, labels), output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HKBpibKWpl7b"
   },
   "outputs": [],
   "source": [
    "class OurBigBirdModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    OurBigBirdModel class is a class for the model that uses the BigBird model and the OrdinalRegressionHead.\n",
    "    It uses the pretrained classifier of a BigBird model and adds an ordinal regression head on top of it.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, bert, num_classes=5, intermediate_layers=None):\n",
    "        super(OurBigBirdModel, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        # The output of the bert model is 768, as it is the output of the last hidden state.\n",
    "        self.classifier = OrdinalRegressionHead(768, num_classes, intermediate_layers=intermediate_layers)\n",
    "\n",
    "        self.loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        \"\"\"\n",
    "        The forward method for OurBigBirdModel class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : tensor\n",
    "            The input tensor, used for bert.\n",
    "        attention_mask : tensor\n",
    "            The attention mask tensor, used for bert.\n",
    "        labels : tensor\n",
    "            The target labels for the classification problem.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple(tensor, tensor)\n",
    "            The loss and the output of the model.\n",
    "            The loss is useful in the training phase, as we are using Trainer from HuggingFace.\n",
    "        \"\"\"\n",
    "        x = self.bert(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        logits, output = self.classifier(x)\n",
    "        return self.loss(logits, labels), output\n",
    "\n",
    "class EmbeddingBigBirdModel(torch.nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(EmbeddingBigBirdModel, self).__init__()\n",
    "        self.bert = bert\n",
    "        # The output of the bert model is 768, as it is the output of the last hidden state.\n",
    "        return\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        The forward method for OurBigBirdModel class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : tensor\n",
    "            The input tensor, used for bert.\n",
    "        attention_mask : tensor\n",
    "            The attention mask tensor, used for bert.\n",
    "        labels : tensor\n",
    "            The target labels for the classification problem.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple(tensor, tensor)\n",
    "            The loss and the output of the model.\n",
    "            The loss is useful in the training phase, as we are using Trainer from HuggingFace.\n",
    "        \"\"\"\n",
    "        return self.bert(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L3T_Y7UFp3Gl"
   },
   "outputs": [],
   "source": [
    "def remove_percentage(df, percent):\n",
    "    \"\"\"\n",
    "    Useful method for development purposes. It removes a percentage of the rows from the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "        The dataframe to remove rows from.\n",
    "    percent : float\n",
    "        The percentage of rows to remove.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        The dataframe with the rows removed.\n",
    "    \"\"\"\n",
    "    if percent == 0:\n",
    "        return df\n",
    "    numpy.random.seed(42)\n",
    "    num_rows_to_remove = int(len(df) * percent)\n",
    "    df_removed = df.sample(frac=1).iloc[num_rows_to_remove:]\n",
    "    return df_removed\n",
    "\n",
    "\n",
    "def convert_label_to_one_hot_encodings(labels: list[float], num_classes, max_label=3500):\n",
    "    \"\"\"\n",
    "    Convert the labels to a class representation. The class representation is a one-hot encoding of the labels.\n",
    "    This preparation is for the ordinal regression problem.\n",
    "\n",
    "    If the one-hot encoding of the label 3 for multi-class classification is [0, 0, 1, 0, 0],\n",
    "      then the one-hot encoding of label 3 for ordinal regression is [1, 1, 0, 0].\n",
    "\n",
    "    To convert from multi-class to ordinal regression, we can do the following trick:\n",
    "        - compute the one-hot encoding of the labels for multi-class classification\n",
    "        - replace all the 0s with 1s until the first 1 is found (for multi-class classification a 1 is found at the index of the class)\n",
    "        - remove the first column of the one-hot encoding (the first 1)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list[float]\n",
    "        The list of labels to convert.\n",
    "    num_classes : int\n",
    "        The number of classes in the classification problem.\n",
    "    max_label : int, optional\n",
    "        The maximum label in the dataset. This is used to normalize the labels. The default is 2800.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The tensor of the one-hot encoding of the labels.\n",
    "    \"\"\"\n",
    "    # reminder: in ordinal regression, class = sum(output > 0.5) + 1\n",
    "\n",
    "    # normalize the labels\n",
    "    labels = numpy.array(labels) / max_label\n",
    "    class_labels = numpy.zeros((len(labels), num_classes - 1))\n",
    "    for i, label in enumerate(labels):\n",
    "        class_labels[i] = numpy.array(\n",
    "            [1 if j / num_classes <= label else 0 for j in range(1, num_classes)])\n",
    "    return torch.tensor(class_labels)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute the metrics for the model. The metrics are accuracy, recall, precision, f1, and neighborhood accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : object\n",
    "        The predictions of the model, wrapped in an object by Hugging Face trainer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The dictionary of the metrics.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "\n",
    "    return _compute_metrics(preds, labels)\n",
    "\n",
    "def _compute_metrics(preds, labels):\n",
    "    threshold = 0.5\n",
    "    target_class = numpy.sum(labels > threshold, axis=-1) + 1\n",
    "    output_class = numpy.sum(preds > threshold, axis=-1) + 1\n",
    "\n",
    "    # compute accuracy, recall, precision, f1 for threshold 0.5\n",
    "    accuracy = accuracy_score(target_class, output_class)\n",
    "    recall = recall_score(target_class, output_class, average='macro', zero_division=0)\n",
    "    precision = precision_score(target_class, output_class, average='macro', zero_division=0)\n",
    "    f1 = f1_score(target_class, output_class, average='macro', zero_division=0)\n",
    "\n",
    "    # compute neighborhood accuracy -- consider accurate all predictions that are off by 1\n",
    "    neighborhood_accuracy = numpy.sum(\n",
    "        numpy.abs(target_class - output_class) <= 1).item() / (len(labels) * 1.0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"neighborhood_accuracy\": neighborhood_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def get_embedding(model, encoding):\n",
    "    \"\"\"\n",
    "    Get the embedding from the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model to get the embeddings from.\n",
    "    encoding : dict\n",
    "        The encoding of the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The tensor of the embeddings.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = torch.tensor(encoding[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(encoding[\"attention_mask\"]).to(device)\n",
    "\n",
    "    batch_size = 10\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    start_time = tm()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(input_ids), batch_size):\n",
    "            batch_input_ids = input_ids[i:i+batch_size]\n",
    "            batch_attention_mask = attention_mask[i:i+batch_size]\n",
    "            embedding = model(batch_input_ids, attention_mask=batch_attention_mask).to(\"cpu\")\n",
    "            embeddings.append(embedding)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            elapsed_time = int(tm() - start_time)\n",
    "            time_per_iter = round(elapsed_time / (i+batch_size), 2)\n",
    "            remaining_time = int((len(input_ids)-i)*time_per_iter)\n",
    "            print(f\"Status {i+batch_size}/{len(input_ids)}  Elapsed: {elapsed_time}s ({time_per_iter}s/it)  Remaining: {remaining_time}s\", end=\"\\r\")\n",
    "    print(\"Embeddings done.\")\n",
    "    del input_ids\n",
    "    del attention_mask\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def save_embedding(embeddings, filename):\n",
    "    \"\"\"\n",
    "    Save the embeddings to a file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : tensor\n",
    "        The tensor of the embeddings.\n",
    "    filename : str\n",
    "        The filename to save the embeddings to.\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pkl.dump(embeddings, f)\n",
    "\n",
    "def load_embedding(filename):\n",
    "    \"\"\"\n",
    "    Load the embeddings from a file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The filename to load the embeddings from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The tensor of the embeddings.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        embeddings = pkl.load(f)\n",
    "    return embeddings\n",
    "\n",
    "def prepare_dataset(embeddings, labels, num_classes=5, max_label=3500):\n",
    "    labels_tensor = convert_label_to_one_hot_encodings(labels, num_classes, max_label=max_label)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    labels_tensor = labels_tensor.to(device)\n",
    "    embeddings = embeddings.to(device)\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "        \"embeddings\": embeddings,\n",
    "        \"labels\": labels_tensor\n",
    "        })\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SOrHoB1JpOwX",
    "outputId": "3b9b17fe-1da5-4d07-fb4d-98d5750abd9e"
   },
   "outputs": [],
   "source": [
    "percentage_to_remove = 0\n",
    "\n",
    "load_embeddings = True\n",
    "\n",
    "train_data = pd.read_csv('./data/AMT10/AMT10_train.csv')\n",
    "val_data = pd.read_csv('./data/AMT10/AMT10_validation.csv')\n",
    "test_data = pd.read_csv('./data/AMT10/AMT10_test.csv')\n",
    "\n",
    "train_data = remove_percentage(train_data, percentage_to_remove)\n",
    "val_data = remove_percentage(val_data, percentage_to_remove)\n",
    "test_data = remove_percentage(test_data, percentage_to_remove)\n",
    "\n",
    "train_texts, train_labels = train_data['description'].tolist(), train_data['rating'].tolist()\n",
    "val_texts, val_labels = val_data['description'].tolist(), val_data['rating'].tolist()\n",
    "test_texts, test_labels = test_data['description'].tolist(), test_data['rating'].tolist()\n",
    "\n",
    "if load_embeddings:\n",
    "    train_embeddings = load_embedding('./data/AMT10/train_embeddings.pkl')\n",
    "    train_embeddings = torch.tensor(train_embeddings)\n",
    "    val_embeddings = load_embedding('./data/AMT10/val_embeddings.pkl')\n",
    "    val_embeddings = torch.tensor(val_embeddings)\n",
    "    test_embeddings = load_embedding('./data/AMT10/test_embeddings.pkl')\n",
    "    test_embeddings = torch.tensor(test_embeddings)\n",
    "else:\n",
    "    model_name = \"google/bigbird-roberta-base\"\n",
    "    tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
    "    model = BigBirdForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "    embedding_model = EmbeddingBigBirdModel(model.bert)\n",
    "    embedding_model.eval()\n",
    "\n",
    "    train_embeddings = get_embedding(embedding_model, train_encodings)\n",
    "    val_embeddings = get_embedding(embedding_model, val_encodings)\n",
    "    test_embeddings = get_embedding(embedding_model, test_encodings)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    save_embedding(train_embeddings.cpu().numpy(), './data/AMT10/train_embeddings.pkl')\n",
    "    save_embedding(val_embeddings.cpu().numpy(), './data/AMT10/val_embeddings.pkl')\n",
    "    save_embedding(test_embeddings.cpu().numpy(), './data/AMT10/test_embeddings.pkl')\n",
    "\n",
    "def get_model_class_from_name(name):\n",
    "    if name == \"OurBigBirdModel\":\n",
    "        return OurBigBirdModel\n",
    "    elif name == \"OrdinalRegressionClassifier\":\n",
    "        return OrdinalRegressionClassifier\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:23<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 23.2966, 'train_samples_per_second': 2256.554, 'train_steps_per_second': 2.575, 'train_loss': 0.6939650217692057, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 250.00it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:22<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 22.5638, 'train_samples_per_second': 2329.838, 'train_steps_per_second': 2.659, 'train_loss': 0.6922763188680013, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 250.09it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:23<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 23.1657, 'train_samples_per_second': 2269.308, 'train_steps_per_second': 2.59, 'train_loss': 0.6928874333699544, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.97it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:25<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 25.3405, 'train_samples_per_second': 2074.543, 'train_steps_per_second': 2.368, 'train_loss': 0.6933565139770508, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 250.18it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:24<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.2504, 'train_samples_per_second': 2167.8, 'train_steps_per_second': 2.474, 'train_loss': 0.6925870895385742, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.33it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:23<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 23.6156, 'train_samples_per_second': 2226.067, 'train_steps_per_second': 2.541, 'train_loss': 0.6929792404174805, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.36it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:24<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.5912, 'train_samples_per_second': 2137.754, 'train_steps_per_second': 2.44, 'train_loss': 0.6934031168619792, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 250.00it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:27<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 27.2202, 'train_samples_per_second': 1931.288, 'train_steps_per_second': 2.204, 'train_loss': 0.6924768447875976, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:24<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.6998, 'train_samples_per_second': 2128.36, 'train_steps_per_second': 2.429, 'train_loss': 0.6929470062255859, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.45it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:23<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 23.8226, 'train_samples_per_second': 2206.729, 'train_steps_per_second': 2.519, 'train_loss': 0.6933706919352214, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 199.79it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:24<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.0591, 'train_samples_per_second': 2185.034, 'train_steps_per_second': 2.494, 'train_loss': 0.6920722961425781, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 211.75it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:23<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 23.9613, 'train_samples_per_second': 2193.954, 'train_steps_per_second': 2.504, 'train_loss': 0.6927413940429688, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.74it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:23<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 23.893, 'train_samples_per_second': 2200.222, 'train_steps_per_second': 2.511, 'train_loss': 0.6932823816935222, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:24<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.8687, 'train_samples_per_second': 2113.898, 'train_steps_per_second': 2.413, 'train_loss': 0.6924648284912109, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.66it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:25<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 25.6428, 'train_samples_per_second': 2050.09, 'train_steps_per_second': 2.34, 'train_loss': 0.6928828557332357, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 199.79it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 60/60 [00:25<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 25.2991, 'train_samples_per_second': 2077.939, 'train_steps_per_second': 2.372, 'train_loss': 0.6933152516682942, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.84it/s]\n",
      "c:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      " 15%|█▌        | 9/60 [00:04<00:22,  2.23it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 72\u001b[0m\n\u001b[0;32m     53\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     54\u001b[0m learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m     55\u001b[0m output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     64\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     65\u001b[0m     model\u001b[38;5;241m=\u001b[39mto_train_model,\n\u001b[0;32m     66\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n\u001b[0;32m     76\u001b[0m eval_metric \u001b[38;5;241m=\u001b[39m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\transformers\\trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\transformers\\trainer.py:2165\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2162\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2164\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2165\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\accelerate\\data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[1;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:2807\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m   2805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m   2806\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2807\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2808\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[0;32m   2809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:2788\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2786\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2787\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2788\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2790\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\datasets\\formatting\\formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\datasets\\formatting\\formatting.py:400\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\datasets\\formatting\\formatting.py:449\u001b[0m, in \u001b[0;36mPythonFormatter.format_batch\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    448\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m--> 449\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32mc:\\Users\\Stefan.saraev\\Anaconda3\\envs\\e2_env\\Lib\\site-packages\\datasets\\formatting\\formatting.py:220\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mdecode_column(column, column_name) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m--> 220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mdecode_batch(batch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m batch\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [\n",
    "        1e-5, \n",
    "        3e-5, \n",
    "        5e-5\n",
    "    ],\n",
    "    'per_device_train_batch_size': [\n",
    "        256\n",
    "    ],\n",
    "    'num_train_epochs': [\n",
    "        200,\n",
    "    ],\n",
    "    \n",
    "    # Model\n",
    "    'model_class': [\n",
    "        'OrdinalRegressionClassifier',\n",
    "    ],\n",
    "    'num_classes': [\n",
    "        5,\n",
    "        10,\n",
    "        35\n",
    "    ],\n",
    "    'intermediate_layers': [\n",
    "        [256, 128, 64, 32],\n",
    "        [512, 128, 32],\n",
    "        [256, 64, 16],\n",
    "    ]\n",
    "}\n",
    "\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "best_eval_metric = float('-inf')\n",
    "best_params = None\n",
    "best_trainer = None\n",
    "\n",
    "for params in param_combinations:\n",
    "    params = dict(zip(param_grid.keys(), params))\n",
    "    learning_rate = params['learning_rate']\n",
    "    train_batch_size = params['per_device_train_batch_size']\n",
    "    num_epochs = params['num_train_epochs']\n",
    "    model_class = get_model_class_from_name(params['model_class'])\n",
    "    num_classes = params['num_classes']\n",
    "    intermediate_layers = params['intermediate_layers']\n",
    "    \n",
    "    to_train_model = model_class(embeddings_size=768, num_classes=num_classes, intermediate_layers=intermediate_layers)\n",
    "\n",
    "    train_dataset=prepare_dataset(train_embeddings, train_labels, num_classes=num_classes)\n",
    "    val_dataset=prepare_dataset(val_embeddings, val_labels, num_classes=num_classes)\n",
    "    test_dataset=prepare_dataset(test_embeddings, test_labels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    learning_rate=learning_rate,\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=train_batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    metric_for_best_model=\"neighborhood_accuracy\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=to_train_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "    eval_metric = eval_results['eval_accuracy']\n",
    "\n",
    "    if eval_metric > best_eval_metric:\n",
    "        best_eval_metric = eval_metric\n",
    "        best_params = params\n",
    "        best_trainer = trainer\n",
    "\n",
    "print(f\"Best Evaluation Metric: {best_eval_metric}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "eval_results = best_trainer.evaluate(test_dataset)\n",
    "print(eval_results)\n",
    "\n",
    "torch.save(best_trainer.model.state_dict(), \"./models/ordinal_regression_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPOki/b/VQ8ups8qMwp7W9q",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
