{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RaduSima/SSL_Project2024/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:44:56.598898Z",
     "start_time": "2024-05-30T19:44:56.594203Z"
    },
    "id": "zTDO-OnwpYbJ"
   },
   "outputs": [],
   "source": [
    "# ! pip install datasets\n",
    "# ! pip install -U accelerate\n",
    "# ! pip install -U transformers\n",
    "\n",
    "import ast\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from architectures import OurDifficultyClassifierModel, OurTagClassifierModel\n",
    "from utils import (compute_metrics_difficulty_classifier,\n",
    "                   compute_metrics_tag_classifier, get_model_class_from_name,\n",
    "                   maybe_load_embeddings,\n",
    "                   prepare_dataset_difficulty_classifier,\n",
    "                   prepare_dataset_tag_classifier,\n",
    "                   prepare_finetune_dataset_difficulty_classifier,\n",
    "                   prepare_finetune_dataset_tag_classifier, save_torch_model,\n",
    "                   transformers_classes, \n",
    "                   create_metrics_difficulty_function,\n",
    "                   create_metrics_tag_function,\n",
    "                   transformers_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:44:56.910250Z",
     "start_time": "2024-05-30T19:44:56.625457Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SOrHoB1JpOwX",
    "outputId": "3b9b17fe-1da5-4d07-fb4d-98d5750abd9e"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/AMT10/AMT10_train.csv')\n",
    "val_data = pd.read_csv('./data/AMT10/AMT10_validation.csv')\n",
    "test_data = pd.read_csv('./data/AMT10/AMT10_test.csv')\n",
    "\n",
    "train_texts, train_difficulty_ratings, train_tags = train_data['description'].tolist(), train_data['rating'].tolist(), train_data['tags'].tolist()\n",
    "val_texts, val_difficulty_ratings, val_tags = val_data['description'].tolist(), val_data['rating'].tolist(), val_data['tags'].tolist()\n",
    "test_texts, test_difficulty_ratings, test_tags = test_data['description'].tolist(), test_data['rating'].tolist(), test_data['tags'].tolist()\n",
    "\n",
    "train_tags = [ast.literal_eval(tags) for tags in train_tags]\n",
    "val_tags = [ast.literal_eval(tags) for tags in val_tags]\n",
    "test_tags = [ast.literal_eval(tags) for tags in test_tags]\n",
    "\n",
    "all_tags = sorted(list(set(itertools.chain.from_iterable(train_tags + val_tags + test_tags))))\n",
    "tag2id = {tag: i for i, tag in enumerate(all_tags)}\n",
    "id2tag = {i: tag for i, tag in enumerate(all_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:44:56.919454Z",
     "start_time": "2024-05-30T19:44:56.911771Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_finetune_tag_classifier(transformer_name, train_texts, train_tags, val_texts, val_tags, test_texts, test_tags, intermediate_layers, tag2id):\n",
    "    base_model = transformers_classes[transformer_name][\"model_class\"].from_pretrained(transformer_name)\n",
    "    embedding_size = transformers_classes[transformer_name][\"embedding_size\"]\n",
    "\n",
    "    train_dataset=prepare_finetune_dataset_tag_classifier(transformer_name, train_texts, train_tags, tag2id)\n",
    "    val_dataset=prepare_finetune_dataset_tag_classifier(transformer_name, val_texts, val_tags, tag2id)\n",
    "    test_dataset=prepare_finetune_dataset_tag_classifier(transformer_name, test_texts, test_tags, tag2id)\n",
    "\n",
    "    model = OurTagClassifierModel(base_model, embedding_size, len(tag2id), intermediate_layers=intermediate_layers)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "      learning_rate=1e-4,\n",
    "      output_dir=\"./results\",\n",
    "      num_train_epochs=100,\n",
    "      per_device_train_batch_size=8,\n",
    "      per_device_eval_batch_size=8,\n",
    "      warmup_steps=500,\n",
    "      weight_decay=0.01,\n",
    "      logging_dir=\"./logs\",\n",
    "      metric_for_best_model=\"f1\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics_tag_classifier\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    eval_results = trainer.evaluate(test_dataset)\n",
    "    print(eval_results)\n",
    "\n",
    "    save_torch_model(\n",
    "        trainer.model,\n",
    "        {\"tag2id\": tag2id, \"intermediate_layers\": intermediate_layers, 'model_class': OurTagClassifierModel},\n",
    "        f\"{transformer_name.replace('/', '_')}_transformer_tag_classifier\"\n",
    "    )\n",
    "    # save scores\n",
    "    with open(f'./results/{transformer_name.replace(\"/\", \"_\")}__transformer_tag_classifier_scores.txt', 'w') as f:\n",
    "        f.write(str(eval_results))\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:44:56.927991Z",
     "start_time": "2024-05-30T19:44:56.920973Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_finetune_difficulty_classifier(transformer_name, train_texts, train_difficulty_ratings, val_texts, val_difficulty_ratings, test_texts, test_difficulty_ratings, intermediate_layers, num_classes):\n",
    "    base_model = transformers_classes[transformer_name][\"model_class\"].from_pretrained(\n",
    "        transformer_name)\n",
    "    embedding_size = transformers_classes[transformer_name][\"embedding_size\"]\n",
    "\n",
    "    train_dataset = prepare_finetune_dataset_difficulty_classifier(\n",
    "        transformer_name, train_texts, train_difficulty_ratings, num_classes=num_classes)\n",
    "    val_dataset = prepare_finetune_dataset_difficulty_classifier(\n",
    "        transformer_name, val_texts, val_difficulty_ratings, num_classes=num_classes)\n",
    "    test_dataset = prepare_finetune_dataset_difficulty_classifier(\n",
    "        transformer_name, test_texts, test_difficulty_ratings, num_classes=num_classes)\n",
    "\n",
    "    model = OurDifficultyClassifierModel(\n",
    "        base_model, embedding_size, num_classes, intermediate_layers)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        learning_rate=1e-5,\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=100,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        metric_for_best_model=\"neighborhood_accuracy\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics_difficulty_classifier\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    eval_results = trainer.evaluate(test_dataset)\n",
    "    print(eval_results)\n",
    "\n",
    "    save_torch_model(\n",
    "        trainer.model,\n",
    "        {\"num_classes\": num_classes, \"intermediate_layers\": intermediate_layers, 'model_class': OurDifficultyClassifierModel},\n",
    "        f\"{transformer_name.replace('/', '_')}_transformer_difficulty_classifier\"\n",
    "    )\n",
    "\n",
    "    # save scores\n",
    "    with open(f\"{transformer_name.replace('/', '_')}_transformer_difficulty_classifier_scores.txt\", \"w\") as f:\n",
    "        f.write(str(eval_results))\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:44:56.942257Z",
     "start_time": "2024-05-30T19:44:56.930Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_grid_search_tag_classifier(transformer_name, train_texts, train_tags, val_texts, val_tags, test_texts, test_tags, metric_compare_grid_results, param_grid):\n",
    "    train_embeddings = maybe_load_embeddings(\n",
    "        transformer_name, train_texts, \"train\", \"./data/AMT10\")\n",
    "    val_embeddings = maybe_load_embeddings(\n",
    "        transformer_name, val_texts, \"val\", \"./data/AMT10\")\n",
    "    test_embeddings = maybe_load_embeddings(\n",
    "        transformer_name, test_texts, \"test\", \"./data/AMT10\")\n",
    "\n",
    "    param_combinations = list(itertools.product(*param_grid.values()))\n",
    "    embedding_size = transformers_classes[transformer_name][\"embedding_size\"]\n",
    "\n",
    "    best_eval_metric = float('-inf')\n",
    "    best_params = None\n",
    "    best_trainer = None\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for params in param_combinations:\n",
    "        params = dict(zip(param_grid.keys(), params))\n",
    "        learning_rate = params['learning_rate']\n",
    "        train_batch_size = params['per_device_train_batch_size']\n",
    "        num_epochs = params['num_train_epochs']\n",
    "        model_class = get_model_class_from_name(params['model_class'])\n",
    "        num_classes = len(tag2id)\n",
    "        intermediate_layers = params['intermediate_layers']\n",
    "        train_metric = params['train_metric']\n",
    "        threshold = params['threshold']\n",
    "        metrics_function = create_metrics_tag_function(threshold)\n",
    "\n",
    "        to_train_model = model_class(\n",
    "            embedding_size=embedding_size, num_classes=num_classes, intermediate_layers=intermediate_layers)\n",
    "\n",
    "        train_dataset = prepare_dataset_tag_classifier(\n",
    "            train_embeddings, train_tags, tag2id)\n",
    "        val_dataset = prepare_dataset_tag_classifier(\n",
    "            val_embeddings, val_tags, tag2id)\n",
    "        test_dataset = prepare_dataset_tag_classifier(\n",
    "            test_embeddings, test_tags, tag2id)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            learning_rate=learning_rate,\n",
    "            output_dir=\"./results\",\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=train_batch_size,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=\"./logs\",\n",
    "            metric_for_best_model=train_metric\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=to_train_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=metrics_function\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        eval_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "        print(eval_results)\n",
    "\n",
    "        eval_metric = eval_results[f'eval_{metric_compare_grid_results}']\n",
    "        \n",
    "        result_entry = params.copy()\n",
    "        for key in eval_results.keys():\n",
    "            result_entry[key] = eval_results[key]\n",
    "        results.append(result_entry)\n",
    "\n",
    "        if eval_metric > best_eval_metric:\n",
    "            best_eval_metric = eval_metric\n",
    "            best_params = params\n",
    "            best_trainer = trainer\n",
    "\n",
    "    print(\n",
    "        f\"Best Evaluation Metric: {metric_compare_grid_results}: {best_eval_metric}\")\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    test_dataset = prepare_dataset_tag_classifier(\n",
    "        test_embeddings, test_tags, tag2id)\n",
    "    best_eval_results = best_trainer.evaluate(test_dataset)\n",
    "    print(best_eval_results)\n",
    "\n",
    "    best_model_hyperparams = {\n",
    "        \"tag2id\": tag2id,\n",
    "        \"transformer_name\": transformer_name,\n",
    "        \"embedding_size\": embedding_size,\n",
    "        \"num_classes\": len(tag2id),\n",
    "        \"intermediate_layers\": best_params['intermediate_layers'],\n",
    "        \"model_class\": best_params['model_class']\n",
    "    }\n",
    "\n",
    "    save_torch_model(\n",
    "        best_trainer.model,\n",
    "        best_model_hyperparams,\n",
    "        f\"{transformer_name.replace('/', '_')}_tag_classifier\"\n",
    "    )\n",
    "    # save scores\n",
    "    with open(f\"{transformer_name.replace('/', '_')}_tag_classifier_scores.txt\", \"w\") as f:\n",
    "        f.write(str(best_eval_results))\n",
    "        \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f\"{transformer_name.replace('/', '_')}_grid_search_tag_results.csv\", index=False)\n",
    "\n",
    "    return best_params, best_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T19:44:56.953916Z",
     "start_time": "2024-05-30T19:44:56.943268Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_grid_search_difficulty_classifier(transformer_name, train_texts, train_difficulty_ratings, val_texts, val_difficulty_ratings, test_texts, test_difficulty_ratings, metric_compare_grid_results, param_grid):\n",
    "    train_embeddings = maybe_load_embeddings(\n",
    "        transformer_name, train_texts, \"train\", \"./data/AMT10\")\n",
    "    val_embeddings = maybe_load_embeddings(\n",
    "        transformer_name, val_texts, \"val\", \"./data/AMT10\")\n",
    "    test_embeddings = maybe_load_embeddings(\n",
    "        transformer_name, test_texts, \"test\", \"./data/AMT10\")\n",
    "\n",
    "    param_combinations = list(itertools.product(*param_grid.values()))\n",
    "    embedding_size = transformers_classes[transformer_name][\"embedding_size\"]\n",
    "\n",
    "    best_eval_metric = float('-inf')\n",
    "    best_params = None\n",
    "    best_trainer = None\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for params in param_combinations:\n",
    "        params = dict(zip(param_grid.keys(), params))\n",
    "        learning_rate = params['learning_rate']\n",
    "        train_batch_size = params['per_device_train_batch_size']\n",
    "        num_epochs = params['num_train_epochs']\n",
    "        model_class = get_model_class_from_name(params['model_class'])\n",
    "        num_classes = params['num_classes']\n",
    "        intermediate_layers = params['intermediate_layers']\n",
    "        train_metric = params['train_metric']\n",
    "        threshold = params['threshold']\n",
    "        metrics_function = create_metrics_difficulty_function(threshold)\n",
    "\n",
    "        to_train_model = model_class(\n",
    "            embedding_size=embedding_size, num_classes=num_classes, intermediate_layers=intermediate_layers)\n",
    "\n",
    "        train_dataset = prepare_dataset_difficulty_classifier(\n",
    "            train_embeddings, train_difficulty_ratings, num_classes=num_classes)\n",
    "        val_dataset = prepare_dataset_difficulty_classifier(\n",
    "            val_embeddings, val_difficulty_ratings, num_classes=num_classes)\n",
    "        test_dataset = prepare_dataset_difficulty_classifier(\n",
    "            test_embeddings, test_difficulty_ratings, num_classes=num_classes)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            learning_rate=learning_rate,\n",
    "            output_dir=\"./results\",\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=train_batch_size,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=\"./logs\",\n",
    "            metric_for_best_model=train_metric\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=to_train_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=metrics_function\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        eval_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "        eval_metric = eval_results[f'eval_{metric_compare_grid_results}']\n",
    "        \n",
    "        result_entry = params.copy()\n",
    "        \n",
    "        for key in eval_results.keys():\n",
    "            result_entry[key] = eval_results[key]\n",
    "        \n",
    "        results.append(result_entry)\n",
    "\n",
    "        if eval_metric > best_eval_metric:\n",
    "            best_eval_metric = eval_metric\n",
    "            best_params = params\n",
    "            best_trainer = trainer\n",
    "\n",
    "    print(\n",
    "        f\"Best Evaluation Metric: {metric_compare_grid_results}: {best_eval_metric}\")\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    test_dataset = prepare_dataset_difficulty_classifier(\n",
    "        test_embeddings, test_difficulty_ratings, num_classes=best_params['num_classes'])\n",
    "    best_eval_results = best_trainer.evaluate(test_dataset)\n",
    "    print(best_eval_results)\n",
    "\n",
    "    best_model_hyperparams = {\n",
    "        \"transformer_name\": transformer_name,\n",
    "        \"num_classes\": best_params['num_classes'],\n",
    "        \"embedding_size\": embedding_size,\n",
    "        \"intermediate_layers\": best_params['intermediate_layers'],\n",
    "        \"model_class\": best_params['model_class']\n",
    "    }\n",
    "\n",
    "    save_torch_model(\n",
    "        best_trainer.model,\n",
    "        best_model_hyperparams,\n",
    "        f\"{transformer_name.replace('/', '_')}_difficulty_classifier\"\n",
    "    )\n",
    "    # save scores\n",
    "    with open(f\"{transformer_name.replace('/', '_')}_difficulty_classifier_scores.txt\", \"w\") as f:\n",
    "        f.write(str(best_eval_results))\n",
    "        \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f\"{transformer_name.replace('/', '_')}_grid_search_difficulty_results.csv\", index=False)\n",
    "    \n",
    "    return best_params, best_eval_results, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-30T19:44:56.954927Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Train grid search difficulty classifier\n",
    "param_grid = {\n",
    "    'learning_rate': [\n",
    "        1e-4,\n",
    "    ],\n",
    "    'per_device_train_batch_size': [\n",
    "        8 # could be bigger (1024, 2048)\n",
    "    ],\n",
    "    'num_train_epochs': [\n",
    "        10, # could be bigger (500, 1000)\n",
    "    ],\n",
    "    \"train_metric\": [\n",
    "        \"neighborhood_accuracy\",\n",
    "        \"accuracy\",\n",
    "    ],\n",
    "\n",
    "    # Model\n",
    "    'model_class': [\n",
    "        'OrdinalRegressionClassifier',\n",
    "    ],\n",
    "    'num_classes': [\n",
    "        5,\n",
    "        10,\n",
    "        35\n",
    "    ],\n",
    "    'intermediate_layers': [\n",
    "        [256, 128, 64, 32],\n",
    "        [512, 128, 32],\n",
    "        [256, 64, 16],\n",
    "    ],\n",
    "    'threshold': [\n",
    "        0.25,\n",
    "        0.35,\n",
    "        0.5,\n",
    "    ]\n",
    "}\n",
    "\n",
    "transformer_name = \"google/bigbird-roberta-large\"\n",
    "# transformer_name = \"t5-small\"\n",
    "\n",
    "best_difficulty_classifier_params, best_difficulty_classifier_eval_results = train_grid_search_difficulty_classifier(transformer_name, train_texts, train_difficulty_ratings, val_texts, val_difficulty_ratings, test_texts, test_difficulty_ratings, \"neighborhood_accuracy\", param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "best_difficulty_classifier_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "best_difficulty_classifier_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Train grid search tag classifier\n",
    "param_grid = {\n",
    "    'learning_rate': [\n",
    "        1e-4,\n",
    "    ],\n",
    "    'per_device_train_batch_size': [\n",
    "        8 # could be bigger (1024, 2048)\n",
    "    ],\n",
    "    'num_train_epochs': [\n",
    "        10, # could be bigger (500, 1000)\n",
    "    ],\n",
    "    \"train_metric\": [\n",
    "        \"accuracy\",\n",
    "        \"f1\"\n",
    "    ],\n",
    "    # Model\n",
    "    'model_class': [\n",
    "        'TagClassifier',\n",
    "    ],\n",
    "    'intermediate_layers': [\n",
    "        [256, 128, 64, 32],\n",
    "        [512, 128, 32],\n",
    "        [256, 64, 16],\n",
    "    ],\n",
    "    'threshold': [\n",
    "        0.25,\n",
    "        0.35,\n",
    "        0.5\n",
    "    ]\n",
    "}\n",
    "\n",
    "transformer_name = \"google/bigbird-roberta-large\"\n",
    "# transformer_name = \"t5-small\"\n",
    "\n",
    "best_tag_classifier_params, best_tag_classifier_eval_results = train_grid_search_tag_classifier(transformer_name, train_texts, train_tags, val_texts, val_tags, test_texts, test_tags, \"f1\", param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "best_tag_classifier_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "best_tag_classifier_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# full_difficulty_classifier_model_eval_results = train_and_finetune_difficulty_classifier(transformer_name, train_texts, train_difficulty_ratings, val_texts, val_difficulty_ratings, test_texts, test_difficulty_ratings, best_difficulty_classifier_params['intermediate_layers'], best_difficulty_classifier_params['num_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# full_tag_classifier_model_eval_results = train_and_finetune_tag_classifier(transformer_name, train_texts, train_tags, val_texts, val_tags, test_texts, test_tags, best_tag_classifier_params['intermediate_layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPOki/b/VQ8ups8qMwp7W9q",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
